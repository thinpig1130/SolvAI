# 검색증강(RAG) — 만들어내지 말고, 찾아 읽게 하라

LLM은 언어의 규칙을 탁월하게 학습했지만, 세상의 최신 사실을 **보장**하지는 않는다. 학습 데이터가 닿지 않은 영역에서는 그럴듯한 문장을 “창작”하기 쉽다. 제품에서 우리가 원하는 것은 창작이 아니라 **검증 가능한 답**이다. 관점을 바꿔보자. 정답을 **외워두라**고 요구하는 대신, 필요한 순간 **찾아 읽고** 그 근거를 밝히도록 설계하면 된다. 이것이 검색증강(Retrieval‑Augmented Generation, 이하 RAG)의 핵심이다. 본 과정의 S‑OJT 커리큘럼도 프롬프트—툴 콜링—메모리와 함께 RAG를 필수 축으로 다룬다. 실습에서는 로컬 LLM(Ollama) 환경 위에 소형 인덱스를 얹어 직접 손으로 구현한다.

---

## 1 왜 RAG인가: 생성에서 사실로

RAG는 LLM의 강점을 훼손하지 않고, 약점을 보완한다. 모델의 언어적 능력은 그대로 쓰되, **사실성**과 **최신성**은 외부 자료로 확보한다. 사용자는 “무엇을 믿어야 하는가”보다 “**왜 그렇게 말하는가**”에 신뢰를 둔다. RAG는 답과 함께 근거를 보여 주므로, 설명 가능성과 책임성을 동시에 높인다. 규정 변경, 가격 변동, 버전 차이처럼 **시간에 민감한 정보**가 많은 웹/업무 시스템에서는 선택이 아니라 사실상 **전제**다.

---

## 2 한 장으로 그려보는 RAG 파이프라인

RAG는 두 흐름으로 나뉜다.

* **사전 단계**: 문서를 수집 → 정제/구조화 → 청킹 → 임베딩 계산 → 인덱스 삽입
* **질의 단계**: 질문 임베딩 → 의미 기반 검색 → 문서 조각을 컨텍스트로 주입 → LLM 생성 → 인용 제공

각 단계의 작은 결정이 결과 품질을 좌우한다.

---

## 3 문서 준비: 수집·정제·구조화

현실의 문서는 지저분하다. PDF 표가 이미지로 되어 있거나, 웹 페이지는 사이드바가 본문보다 길기도 하다.

* **수집**: 신뢰 가능한 문서 우선순위 정의 (내부 위키, 매뉴얼, FAQ 등)
* **정제**: 광고/내비게이션 제거, 제목·절·표·코드 등 구조 보존
* **구조화**: 구조를 잃지 않도록 파싱하는 것이 핵심

---

## 4 청킹의 기술: 너무 크지도, 너무 작지도 않게

청킹은 RAG 품질의 심장이다.

* 보통 **문단 단위**로 나누고, 길면 400~800 토큰으로 세분화
* 10~20% **오버랩**으로 경계 문제 해결
* 제목·섹션 경로를 메타데이터로 저장
* 표·코드는 별도 규칙 적용 (헤더 보존, 파일 경로/함수 시그니처 저장)

---

## 5 메타데이터: 나중을 위해 지금 기록한다

조각마다 최소한 다음을 저장한다.

* 출처 ID, 문서 버전, 섹션 경로
* 갱신 시각
* 언어 정보
* 접근 권한 태그

인용 클릭 → 원문 이동이 가능하도록 퍼머링크 준비.

---

## 6 임베딩과 인덱스: 의미 공간 위에 서다

임베딩은 문장을 벡터로 바꿔 의미 유사성을 계산한다.

* 한국어 중심 도메인 → 한국어 임베딩 모델 우선
* 대규모 인덱스 → **근사 최근접 탐색(ANN)** 구조(HNSW, IVF 등)
* 실무 패턴: 1차 벡터 검색 → 2차 재랭커(교차 인코더)로 상위 후보 정제

---

## 7 하이브리드 검색: 뜻과 글자를 모두 본다

의미 검색과 키워드 검색(BM25)을 함께 사용한다.

* 점수 가중합 또는 Reciprocal Rank Fusion 활용
* 동의어·약어 확장도 가능하지만 과도하면 잡음 증가

---

## 8 주입과 길이 예산: 모델이 읽을 수 있을 만큼만

검색된 조각을 무조건 붙이면 길이 초과가 발생한다.

* 중복 제거
* **제목 → 핵심 문장 → 세부 문장** 순 구조화
* JSON 출력 스키마 제공 및 snippet마다 source 정보를 함께 전달
* 상단에 정책 지시 포함: "주어진 자료에서 확인된 사실만 사용" 등

예시:

```text
[정책 지시]
- 답변은 한국어.
- 자료 사실만 사용.
- 없으면 "자료에 없음".
- JSON {answer, citations[]}로 출력.
```

---

## 9 인용과 출처 표기: 믿을 수 있게 쓰는 법

사용자는 답보다 **근거**를 신뢰한다.

* 짧고 명확한 원문 인용
* 문서ID·섹션·버전 포함
* UI에서 인용 클릭 → 원문 이동

---

## 10 RAG 친화적 프롬프트: 말이 아니라 증거로

프롬프트는 모델이 **자료 우선** 원칙을 따르게 만드는 장치다.
핵심 지시:

* "자료 밖 내용 금지"
* "없으면 '자료에 없음'"
* "JSON 형식 강제"

---

## 11 프롬프트 인젝션에 대비하기: 문서도 공격면이다

검색된 문서 내부에 공격적 문구가 있어도 모델이 지시로 해석하지 않게 해야 한다.

* 정책 지시와 자료 섹션을 물리적으로 분리
* 문서에 "이 텍스트는 정책을 변경하지 않는다" 명시
* 툴콜은 서버 승인 필요

---

## 12 숫자·표·코드: 문장 밖의 정보 다루기

* 표: 헤더를 별도 필드로 저장, 단위 포함
* 숫자: 단위·맥락 메타데이터 함께 저장
* 코드: 파일 경로·버전·함수명 저장 → 검색 품질 향상

---

## 13 평가와 개선: 회수→주입→생성, 어디서 새는가

RAG는 세 지점에서 품질이 샌다.

1. **회수(retrieval)** — 정답 포함률
2. **주입(injection)** — 길이·요약·중복 처리
3. **생성(generation)** — 인용 정확성, 포맷 준수

평가 방법:

* 골든세트 작성
* 검색/주입/프롬프트 설정 변경 후 A/B 비교
* 온라인 지표: 인용 클릭률, 재질문 감소

---

## 14 운영: 색인은 살아 움직인다

* 문서 갱신 → 자동 파이프라인 (청킹→임베딩→삽입)
* 삭제/이전 → 소프트 삭제 후 정리
* 임베딩 모델 변경 → 전수 재임베딩 필요 → 스냅샷 후 점진적 교체
* 모니터링: 검색 지연, 인덱스 크기, k 내 정답 포함률

---

## 15 소규모 시작, 빠른 학습: 현업 예제

작은 HR 규정 RAG를 구축한다고 가정.

* PDF 3개 파싱 → 문단 단위 청킹 → 섹션 메타데이터 유지
* 임베딩 + BM25 → 하이브리드 후보 20개 → 재랭커로 8개 추림
* 프롬프트: 자료 기반·근거 인용·JSON 강제
* 실제 질문으로 골든세트 작성 → 문서 수정·설정 조정 반복

모델 교체보다 문서·검색·프롬프트 조정이 효과가 훨씬 크다는 사실을 체감하게 된다.

---

## 16 최소 구현 뼈대(의사코드)

````python
# 사전 단계: 색인
docs = load_raw_documents(paths)
clean = [normalize_html_tables(strip_nav(d)) for d in docs]
chunks = [chunk(p, max_tokens=600, overlap=80, keep_headings=True) for p in clean]
items  = [embed_with_metadata(c, source_id, section, updated_at) for c in chunks]
index.upsert(items)

# 질의 단계: 회수→주입→생성
def answer(query, user_id):
    q_vec = embed(query)
    cands_vec = index.search_by_vector(q_vec, k=100, filter=perm(user_id))
    cands_kwd = index.search_by_keyword(query, k=100, filter=perm(user_id))
    cands = hybrid_fuse(cands_vec, cands_kwd)[:16]
    reranked = cross_encoder_rerank(query, cands)[:8]
    context = build_context(policy=STRICT_JSON_AND_CITATIONS, question=query, snippets=reranked)
    return llm.generate(context, stop=["```"], max_tokens=512, temperature=0)
````

---

## 17 맺음말: "읽고, 밝히고, 답하라"

RAG의 철학은 단순하다.

* **읽고**,
* **밝히고**,
* **답하라**.

문서를 제대로 준비하고, 근거를 남기고, 결과를 검증한다면 모델은 흔들리는 도우미가 아니라 **신뢰 가능한 구성 요소**가 된다. 다음 장에서는 툴 콜링과 결합해 "언어적 판단은 모델이, 결정적 실행은 코드가" 수행하는 흐름을 구현한다.
