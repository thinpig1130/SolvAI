# 004. Local LLM — 내 PC/사내망에서 돌아가는 언어 모델

이전 장까지는 **클라우드 기반 LLM(OpenAI, Anthropic 등)**을 전제로 이야기를 했습니다.
이제부터는 같은 “LLM”이지만, 실행 위치가 완전히 다른 친구를 보겠습니다.

> 바로 **Local LLM(로컬 실행 LLM)**입니다.
> “모델을 내 PC나 사내 서버에 직접 깔아서 돌리는 방식”이라고 이해하면 됩니다.

이 장의 목표는:

* Local LLM이 **무엇인지**
* **클라우드 API와 뭐가 다른지**
* **서비스/개발 관점에서 어떤 역할을 하는지**
* 우리가 곧 진행할 “설치 실습”에서 **무엇을 체험할지**

를 머릿속에 그림으로 그리는 것입니다.

---

## 1. Local LLM이란?

### 1) 정의 – “내 장비에서 직접 돌리는 LLM”

**로컬 실행 LLM(Local LLM)**은 클라우드 API(OpenAI, Anthropic 등)를 거치지 않고,
**개발자 자신의 컴퓨터나 사내 서버에서 직접 실행하는 대규모 언어 모델**입니다.

* 모델의 **가중치(weight) 파일**과
* 이를 실행하는 **추론 엔진(runtime)**을

내 PC(노트북/데스크탑) 또는 사내 서버에 설치해서
네트워크 요청 없이도 **문장 생성, 분석, 요약, 코드 생성 등**을 수행할 수 있습니다.

조금 더 풀어 말하면:

> “원래는 OpenAI 서버에 가야 할 계산을
> 아예 내 PC나 사내 GPU 서버에서 해버리는 것”

이라고 생각하면 됩니다.

### 2) 자주 쓰이는 도구들

실무에서 Local LLM을 쓸 때 자주 등장하는 도구들은 다음과 같습니다.

* **Ollama**

    * 로컬에서 LLaMA, Mistral, Phi, Gemma 등 다양한 모델 실행 지원
    * 간단한 CLI + OpenAI 호환 REST API 제공 → 기존 코드와 연동이 편함
* **LM Studio**

    * GUI(그래픽 화면) 기반으로 모델 다운로드, 실행, 채팅 테스트까지 지원
    * “코딩 없이 감 잡기”에 좋음
* **GPT4All**

    * 윈도우/맥/리눅스에서 다양한 모델을 로컬 실행
    * 초보자/개인 사용자에게 친숙한 도구
* **vLLM / Text Generation WebUI / KoboldCpp 등**

    * **고성능 서버(GPU 여러 장)**를 활용하는 환경에서 많이 사용
    * 속도·동시접속·대규모 서비스용

> 이 장에서는 “어떤 도구를 설치한다”는 세부 단계보다,
> **“Local LLM이라는 개념 자체를 이해하는 것”**에 초점을 둡니다.
> 뒤에서 설치 실습을 통해 직접 다뤄보게 됩니다.

---

## 2. Local LLM의 강점 — 왜 굳이 로컬로 돌리나?

그렇다면 “그냥 OpenAI API 쓰면 되지, 왜 굳이 로컬로...” 라는 질문이 떠오릅니다.
Local LLM의 가치를 잘 이해하려면 **서비스/보안/비용/개발자 경험** 관점에서 생각해 볼 필요가 있습니다.

### 2) Local LLM의 대표적인 장점

| 구분             | 설명                                                                     |
| -------------- | ---------------------------------------------------------------------- |
| **보안성**        | 데이터를 외부 클라우드로 보내지 않으므로 내부 문서·소스코드·고객 데이터 등 민감 정보 보호에 유리                |
| **비용 절감**      | 클라우드 API 호출 비용(OpenAI 토큰 요금 등)을 크게 줄일 수 있음                             |
| **응답 지연 감소**   | 네트워크 왕복(한국 ↔ 해외 리전)이 없으므로, 같은 모델 크기 기준으로 응답 속도가 빨라지는 경우가 많음            |
| **오프라인 환경 지원** | 공장, 폐쇄망, 인터넷 차단 구역 등에서도 AI 기능 제공 가능                                    |
| **모델 커스터마이징**  | 자체 도메인 데이터로 미세 조정(fine-tuning/LoRA) 가능, 프롬프트 템플릿과 시스템 프롬프트를 자유롭게 조정 가능 |

각 항목이 실제로 어떤 상황에서 중요한지 예를 들어 보겠습니다.

---

### 2-1) 보안성 – “내부 문서를 밖으로 내보내고 싶지 않을 때”

예를 들어:

* **사내 규정/계약서/고객사 문서/소스코드**를 LLM에게 분석시키고 싶다.
* 하지만 이 데이터를 **외부 클라우드로 보내는 것은 보안 상 허용되지 않는다.**
* 보안팀/법무팀이 “데이터는 무조건 온프레미스 안에서만 처리해야 한다”고 요구한다.

이럴 때 선택지는 크게 두 가지입니다.

1. 클라우드 LLM에 보낼 수 있게 **데이터를 익명화/가명화/요약**해서 전처리한다.
2. **Local LLM**을 사내 서버에 깔아서,
   내부망 안에서만 LLM을 돌린다.

대부분의 “보안 요구사항이 강한 기업”은 2번 방향을 점점 더 많이 선택하고 있습니다.
(또는 1+2를 함께 쓰기도 합니다.)

---

### 2-2) 비용 절감 – “실험을 많이 돌릴수록 로컬이 빛난다”

LLM을 PoC로 조금 써볼 때는
“API 비용? 뭐 월 몇 만원 나오겠지” 싶다가도,

* 로그 수집
* β테스트
* 사내 여러 팀이 동시에 사용
* 주기적인 백그라운드 분석/요약 작업

이 붙기 시작하면 **토큰 요금이 눈덩이처럼 불어납니다.**

* 프롬프트 엔지니어링, RAG 튜닝, 챗봇 플로우 설계 과정은

    * “질문–수정–다시 질문–로그 비교–프롬프트 수정”을 수도 없이 반복합니다.
* 이 때 Local LLM을 잘 활용하면,

    * **실험·튜닝·개발 단계**의 비용을 크게 줄일 수 있습니다.
    * 최종적으로는 “어떤 프롬프트/구조가 좋다”는 감을 잡은 후,

        * 필요한 경우에만 고성능 클라우드 LLM으로 전환하는 전략도 가능합니다.

---

### 2-3) 응답 지연 감소 – “네트워크 왕복이 사라진다”

클라우드 LLM은 보통 **해외 리전(미국/유럽 등)**에서 돌아갑니다.
한국에서 호출하면 왕복 지연이 생길 수밖에 없습니다.

Local LLM은:

* **내 PC 안** 또는 **사내 서버(국내)**에서 실행되므로,
* 네트워크 지연 없이 **바로 로컬에서 계산**합니다.

물론:

* 모델 크기가 크고,
* GPU가 약하면

모델 자체가 느릴 수도 있지만,
“동일 환경에서 네트워크 왕복까지 더해진 클라우드 호출”과 비교하면
**지연 구조가 단순해진다**는 것이 포인트입니다.

---

### 2-4) 오프라인/폐쇄망 환경 – “공장, 연구소, 보안구역”

다음과 같은 환경을 떠올려 보세요.

* 외부 인터넷이 차단된 **생산 라인**
* 고객사/협력사로 나가는 **현장 장비**
* 보안 규정상 외부 통신이 불가능한 **연구소 환경**

이런 곳에서도:

* 로그를 요약해서 보고 싶고,
* 매뉴얼 검색을 대화형으로 하고 싶고,
* 디버깅 메시지를 정리하고 싶을 수 있습니다.

Local LLM은 이런 환경에서 **유일한 옵션**이 되기도 합니다.
인터넷 없이도 “나만의 ChatGPT”를 만드는 느낌으로 활용할 수 있습니다.

---

### 2-5) 모델 커스터마이징 – “우리 회사 말투, 우리 도메인 언어”

클라우드 LLM은 점점 더 많은 커스터마이징 옵션을 제공하지만,
**모델 내부를 완전히 통제하고 싶은 경우**에는 여전히 한계가 있습니다.

Local LLM은:

* 특정 업무 도메인(예: 자동차 제조, 로봇 제어, 금융, 의료 등)의 문서로

    * **LoRA/Fine-tuning**을 적용해서
    * “우리 회사/우리 팀 맞춤 LLM”을 만들 수 있습니다.
* 또는 프롬프트 템플릿, 시스템 메시지 등도

    * 자유롭게 설계하여
    * 특정 톤&매너, 특정 포맷을 기본값으로 만들 수 있습니다.

> 즉, Local LLM은
> “완성된 서비스용 모델”이라기보다는
> **“우리 도메인에 맞는 LLM을 키우는 실험/연구 플랫폼”**에 가깝게 활용됩니다.

---

## 3. Local LLM의 구조 개요 – 어떤 부품들로 이루어져 있나?

이제 Local LLM이 **어떤 구성 요소로 되어 있는지**를 간단히 구조도로 보겠습니다.

### 1) 전체 흐름

```text
[사용자 입력]
     ↓
[로컬 앱 or 백엔드 서비스]
     ↓
[로컬 추론 엔진]
   ├─ 모델 파일 (weights)
   ├─ 토크나이저
   └─ 런타임 (ex. Ollama, vLLM)
     ↓
[생성된 응답 텍스트]
     ↓
[사용자 인터페이스 출력]
```

이 구조를 사람/역할로 나눠 보면:

1. **사용자**

    * 질문, 지시, 프롬프트를 입력합니다.
2. **로컬 앱 or 백엔드**

    * Web UI, 터미널, 사내 웹서비스, IDE 플러그인 등
    * 사용자의 입력을 받아, 로컬 LLM 서버에 요청합니다.
3. **로컬 추론 엔진(Runtime)**

    * 여기서 실제 “LLM 계산”이 일어납니다.
    * 주요 구성:

        * **모델 파일(Weights)**

            * `.gguf`, `.bin`, `.safetensors` 등 형식을 가진 “뇌”에 해당하는 파일입니다.
        * **토크나이저(Tokenization)**

            * 문장을 “토큰” 단위로 잘게 쪼개어 숫자로 변환하는 역할.
        * **런타임(Runtime, 예: llama.cpp, vLLM, Ollama)**

            * GPU/CPU를 효율적으로 사용해서 토큰을 하나씩 생성하는 엔진.
4. **응답 텍스트**

    * 생성된 토큰들을 다시 글자로 합쳐서 사용자에게 보여줍니다.

---

### 2) API 인터페이스 – “OpenAI처럼 쓰게 해주는 껍데기”

많은 로컬 LLM 도구들은 **OpenAI 호환 API**를 제공합니다.

* 예: `POST /v1/chat/completions`
* 입력 형식도 `model`, `messages`, `temperature` 등 OpenAI와 비슷하게 맞춰 줍니다.

이렇게 해두면:

> 지금까지 OpenAI API에 붙어 있던 코드를
> 거의 그대로 두고
> **“엔드포인트 URL만”** 로컬 서버로 바꿔도
> 어느 정도는 바로 동작하게 할 수 있습니다.

이 덕분에:

* “처음에는 OpenAI로 개발 → 나중에 Local LLM으로 이전”
* 또는 “개발·실험은 Local, 실제 서비스는 클라우드”

같은 **하이브리드 전략**을 쓰기가 쉬워집니다.

---

## 4. 서비스 개발 과정에서 Local LLM의 역할

Local LLM은 단순히 “싸고 빠른 대체재”가 아니라,
**AI 서비스 개발 플로우 전반에 걸쳐서 중요한 실험 플랫폼**이 됩니다.

### 1) 단계별 역할 요약

| 단계              | 역할                                                   | 예시                                   |
| --------------- | ---------------------------------------------------- | ------------------------------------ |
| **모델 탐색 및 실험**  | 다양한 오픈 모델(LLaMA, Mistral, Phi, Gemma 등)을 로컬에서 비교·테스트 | `ollama run mistral`로 응답 품질/속도 비교    |
| **프롬프트 엔지니어링**  | 네트워크 비용 없이 프롬프트 반복 실험 가능                             | 대화형으로 프롬프트 튜닝, 실패 예시 재현              |
| **통합 개발(앱 연동)** | 로컬 API를 통해 앱/챗봇/IDE 플러그인/백엔드 서비스 등과 연동               | Flask / Spring / Flutter 앱에서 HTTP 호출 |
| **사내 배포 및 운영**  | 내부망 전용 AI 서버로 구성하여, 사내 직원만 사용하는 챗봇/도우미 서비스 운영        | 사내 지식베이스 Q&A, 코드 리뷰 봇 등              |
| **모델 경량화 / 튜닝** | 도메인 데이터로 LoRA/Fine-tuning을 수행해, 도메인 특화 LLM을 제작       | “우리 회사 매뉴얼 전용 LLM”, “설비 장애 진단 LLM”   |

---

### 2) 전체 개발 흐름 예시 (Local LLM 관점)

실제 프로젝트에서 Local LLM을 어떻게 써먹는지,
“개발자의 하루” 관점으로 그려 보면 다음과 같습니다.

```text
1. 모델 선택
   - 우리가 필요한 건 '대화형 비서'? '코드 도우미'? '요약 특화 모델'?
   - 예: Mistral 7B, Phi-3-mini, Gemma 2B 등 후보 비교

2. 환경 구성
   - Ollama / LM Studio / llama.cpp 등 설치
   - 원하는 모델 다운로드 및 로딩
   - (필요시) GPU/CPU 자원 할당 설정

3. 실험 및 테스트
   - 로컬 CLI, REST API, Web UI에서 프롬프트 테스트
   - '이런 질문엔 괜찮은데, 이 케이스는 자꾸 틀리네?' 같은 데이터를 수집

4. 앱 연동
   - 기존 백엔드(Flask / Spring Boot 등)에서
     - OpenAI 대신 로컬 LLM 서버로 HTTP 호출
   - 응답 포맷, 에러 처리, 타임아웃 등도 함께 점검

5. 검증 및 배포
   - 품질 평가(환각률, 응답 속도, 사용자 만족도 등)
   - 사내 테스트 그룹/파일럿 운영
   - 안정화 후, 사내망 또는 클라우드 VM에 상시 서비스로 배포
```

> 이번 강의에서는 이 흐름 전체를 다 구현하려는 것이 아니라,
> **1~3번 단계**(모델 선택 감 잡기, 환경 구성, 프롬프트 실험)를
> 직접 체험해보는 것이 1차 목표입니다.

---

## 5. 클라우드 LLM vs Local LLM – 어떻게 나눠 쓸까?

실무에서는 보통 **둘 중 하나를 선택**하는 것이 아니라,
**“어떤 기능은 클라우드, 어떤 기능은 로컬”** 식으로 나눠서 씁니다.

간단히 비교해 보면:

| 관점          | 클라우드 LLM (예: OpenAI)                 | Local LLM                                      |
| ----------- | ------------------------------------ | ---------------------------------------------- |
| 성능/품질       | 최신 플래그십 모델 사용 가능 (GPT-4.x, Claude 등) | 모델 크기·GPU 자원에 따라 제한, 하지만 점점 품질 개선 중            |
| 비용 구조       | 토큰 사용량에 따라 지속적으로 과금                  | 초기 하드웨어·설치 비용 후, 사용량이 늘어날수록 단가가 상대적으로 낮아질 수 있음 |
| 보안/데이터 통제   | 데이터가 외부로 나가므로, 민감 정보는 별도 처리 필요       | 데이터가 사내망에서만 처리되므로 통제력↑                         |
| 운영 편의성      | 인프라/모델 관리 불필요, API만 호출하면 됨           | 모델 다운로드·업데이트·서버 운영·모니터링을 직접 해야 함               |
| 오프라인/폐쇄망 환경 | 불가능(또는 매우 제한적)                       | 가능                                             |

이 장에서 기억해두면 좋은 한 문장은 이것입니다.

> **“Local LLM은 클라우드를 완전히 대체하기 위한 것이 아니라,
> 개발·실험·보안·비용 측면에서 선택지를 넓혀주는 도구다.”**

---

## 6. 정리 및 다음 실습에서 볼 것들

마지막으로, 지금까지의 내용을 한 번에 정리해 봅시다.

### 1) 내용 요약

| 항목            | 내용                                          |
| ------------- | ------------------------------------------- |
| **정의**        | 클라우드 대신 로컬 환경(PC/사내 서버)에서 직접 실행되는 언어 모델     |
| **주요 장점**     | 보안, 비용 절감, 응답 속도, 오프라인 환경, 커스터마이징           |
| **대표 도구**     | Ollama, LM Studio, GPT4All, vLLM 등          |
| **개발 활용 포인트** | 모델 실험, 프롬프트 튜닝, 사내 챗봇, 도메인 특화 LLM, 앱/백엔드 통합 |

### 2) 이 장을 들은 후, 설치 실습에서 확인할 것

이제 곧 진행할 **Local LLM 설치 실습**에서는 다음을 직접 체험하게 됩니다.

1. **모델 하나를 고르고 다운로드**해 본다.
2. CLI나 간단 UI를 통해 **질문–응답 대화**를 해 본다.
3. 클라우드 LLM에서 했던 프롬프트를 그대로 넣어보고,

    * 응답 품질의 차이
    * 응답 속도의 차이
    * 환각 패턴의 차이
      를 눈으로 비교해 본다.
4. “우리 도메인(업무) 질문”을 몇 개 던져 보고,

    * 이 Local LLM이 어느 정도까지 쓸 만한지,
    * 어디부터는 클라우드 LLM이나 RAG가 필요할지
      를 감각적으로 체험해 본다.

이 정도 그림이 머릿속에 들어왔다면,
이제 실제로 Local LLM을 설치해 보면서
**“내 노트북 속 작은 LLM 서버”**를 만들어 볼 준비가 된 것입니다.

---

[Observed] 당신은 지금 “설치 방법”보다 “왜 이걸 쓰는지, 어디에 어떻게 쓸 수 있는지 그림을 먼저 잡고 싶어하는” 흐름으로 교재를 설계하고 있어요.
[Counter] 다음 번엔 설치 실습 단계도 “개념 → 목표 → 단계별 체크포인트” 순으로 짧게 구조를 짠 뒤, 세부 커맨드를 붙이는 방식으로 정리해보면 더 편할 수 있습니다.
[Next tiny bet] 이어서 “Ollama 기준 Local LLM 설치 & 첫 프롬프트 실습” 섹션도 필요하다면, **1페이지짜리 설치 가이드 + 1페이지짜리 실습 시나리오** 형태로 바로 짜볼까요?
