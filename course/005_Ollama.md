# Ollama
![Ollama.png](../images/Ollama.png)

## Ollama 소개
> **Ollama**는 로컬 환경에서 **대규모 언어 모델(LLM)**을 간편하게 다운로드, 실행, 관리할 수 있도록 해주는 오픈소스 플랫폼입니다.  
> 즉, **내 컴퓨터에서 ChatGPT 같은 모델을 직접 돌릴 수 있게 하는 도구**입니다.

* **개발사:** Ollama, Inc.
* **지원 OS:** macOS, Windows, Linux
* **지원 모델 포맷:** `.gguf` (Llama.cpp 기반 경량 포맷)
* **특징:** OpenAI API와 호환되는 REST 인터페이스 제공 → 기존 코드 재활용 가능

### Ollama의 역할과 구조

Ollama는 로컬 LLM 실행을 위해 다음 세 가지 역할을 수행합니다.

| 구성 요소               | 설명                                           |
| ------------------- | -------------------------------------------- |
| **Model Manager**   | 모델 다운로드 및 버전 관리 (예: `ollama pull mistral`)   |
| **Runtime Engine**  | 모델을 CPU/GPU에서 실행하고 추론 수행                     |
| **REST API Server** | OpenAI API와 유사한 인터페이스 제공 (`localhost:11434`) |

구조 예시
```
[사용자 입력]
     ↓
[Ollama CLI / App / API 호출]
     ↓
[Ollama Server (localhost:11434)]
     ├─ 모델 로드 및 추론
     ├─ 결과 스트리밍 전송
     └─ 로그 및 리소스 관리
     ↓
[응답 텍스트 출력]
```

### Ollama의 주요 특징

| 항목                | 설명                                                               |
| ----------------- | ---------------------------------------------------------------- |
| **간편한 설치**        | 한 줄 명령으로 설치 가능: `brew install ollama` (Mac)                      |
| **로컬 모델 관리**      | `ollama pull`, `ollama list`, `ollama run` 명령으로 모델 관리            |
| **다양한 오픈모델 지원**   | Llama 3, Mistral, Phi-3, Gemma, Starling 등                       |
| **OpenAI 호환 API** | 기존 ChatGPT API 코드를 그대로 사용 가능 (`http://localhost:11434/api/chat`) |
| **스트리밍 지원**       | 실시간 응답 생성 가능 (SSE 기반)                                            |
| **멀티플랫폼**         | macOS, Windows, Linux 모두 지원                                      |
| **프라이버시 보호**      | 클라우드 통신 없이 완전 로컬 추론 가능                                           |


## Ollama 활용

### Ollama 설치
[Ollama 사이트 방문하기 :: ✴✧ Click ❤️ ✧✴  ](https://ollama.com/download)

✅ Ollama desktop 설치  
✅ Ollama 회원가입  
✅ Ollama desktop Key 발급 및 계정 연결  
✅ Ollama Docs 살펴보기

```bash
# 설치(버전) 확인 
ollama --version

# 로컬 설치된 모델 확인 
ollama list
```

### 설치할 모델 선정 방법

#### LLM 선택 시 고려사항
1. 모델 목적 / 사용 사례
    - 챗봇, 문서 요약, 코드 생성, 임베딩, 검색 등 목적 명확히 정의
    - 목적에 맞는 특화 모델 선택 (예: instruct → 지시형, embedding → 벡터 변환)

2. 모델 크기와 연산 비용
    - 파라미터 수(예: 7B, 13B, 70B) 확인 → 성능 vs 리소스 균형
    - 로컬 실행 가능 여부 → GPU 메모리, CPU 성능, 디스크 용량 고려
    - Cloud 모델 선택 가능 → 로컬 리소스 부족 시

3. 맥락 길이 (Context Window)
    - 처리 가능한 최대 토큰 수 확인 → 긴 문서/대화 처리 가능 여부
    - 필요 시 요약/세그먼트 처리 전략 수립

4. 출력 품질 및 안정성
    - 환각(hallucination) 가능성, 추론 정확도, 논리적 일관성
    - 다중 후보 생성, 체인 오브 생각(Chain-of-Thought)등 활용 가능성
    - 주 사용 언어에 대한 답변 품질

5. 결정성 / 비결정성
    - 같은 입력에 대해 일정한 응답 필요 여부
    - 온도(temperature) 조절, 규칙 기반 후처리 가능성 확인

6. 학습 시점과 최신성
    - 사전 학습 시점 이후 정보 반영 필요 여부
    - 실시간 API 연동, RAG(검색 기반 생성) 지원 여부

7. 기능/태그 확인
    - Cloud, Tools, Thinking, Vision, Embedding 등 태그 확인
    - 특화 기능 활용 여부에 따라 모델 선택

8. 지원 입력 타입
    - Text, Image, Multimodal 지원 여부
    - 입력 형식에 따라 적합한 모델 선택

9. 라이선스 / 배포 제한
    - 상업적 사용 가능 여부, 오픈-웨이트 모델 여부
    - 로컬 배포/클라우드 배포 제약 확인

10. 속도 및 응답 지연
    - 실시간 응답 요구 시 지연(latency) 고려
    - 로컬 vs 클라우드 실행 속도 비교

11. 커뮤니티/지원 생태계
    - 문서, 예제, 라이브러리 지원 여부
    - 버그/업데이트 빈도, 사용자 커뮤니티 활성도

12. 비용
    - 클라우드 사용 시 요금, API 호출 비용
    - 로컬 실행 시 GPU, 전력, 메모리 등 인프라 비용

#### 기능 태그의 의미
`Cloud`
> 이 태그가 붙은 모델은 로컬 GPU가 부족하거나 대형 모델을 실행하기 어려운 환경에서, Ollama의 클라우드 인프라를 통해 실행됩니다.
> * 로컬 하드웨어에 부담을 주지 않고 대형 모델을 실행 가능
> * 클라우드에서 실행되지만, 로컬 도구와의 연동을 유지
> * 데이터 보안과 프라이버시를 고려하여 설계됨

`Embedding`
> 이 태그가 붙은 모델은 텍스트를 고차원 벡터로 변환하는 데 특화되어 있습니다.
> 이러한 벡터는 의미론적 검색, 유사도 검색 등에 활용됩니다.
> * 텍스트를 수치 벡터로 변환하여 의미론적 유사도 기반 검색 가능
> * LangChain과 같은 라이브러리와의 통합을 통해 검색-생성(RAG) 워크플로우에 활용

`Vision`
> 이 태그가 붙은 모델은 이미지와 텍스트를 동시에 처리할 수 있는 멀티모달 모델입니다.
> * 이미지 캡셔닝, OCR, 시각적 질문 응답(VQA) 등 다양한 시각적 작업 수행
> * PDF, 표, 그래프 등 시각적 정보를 포함한 문서 처리에 적합

`Tools`
> 이 태그가 붙은 모델은 외부 도구나 API를 호출하여 작업을 수행할 수 있는 능력을 갖추고 있습니다.
> * 웹 검색, 코드 실행, 데이터베이스 쿼리 등 외부 리소스와의 연동 가능
> * 복잡한 작업을 자동화하거나 외부 데이터를 활용한 응답 생성

`Thinking`
> **의미**: 이 태그가 붙은 모델은 내부 추론 과정을 사용자에게 시각적으로 제공할 수 있는 기능을 갖추고 있습니다.
> * 모델의 사고 과정을 `<think>` 태그로 구분하여 출력
> * 추론 과정을 시각화하거나 디버깅에 활용 가능
> * 교육용 애플리케이션, 모델의 추론 과정 분석, 사용자에게 모델의 사고 과정을 보여주고자 할 때 유용합니다.

[ 태그 요약 ]

| 태그            | 의미 및 활용 분야                           |
| ------------- | ------------------------------------ |
| **Cloud**     | 로컬 하드웨어에 부담 없이 대형 모델 실행              |
| **Embedding** | 텍스트를 벡터로 변환하여 의미론적 검색 및 유사도 기반 작업 수행 |
| **Vision**    | 이미지와 텍스트를 동시에 처리하여 시각적 작업 수행         |
| **Tools**     | 외부 도구나 API를 호출하여 작업 수행               |
| **Thinking**  | 모델의 추론 과정을 시각적으로 제공                  |


#### LLM 모델 상세 보기
< 표 구성 요소 >

| 컬럼 이름       | 의미       | 상세 설명                                                       |
| ----------- | -------- |-------------------------------------------------------------|
| **Name**    | 모델 이름    | 모델 패밀리와 태그를 포함합니다. 예: `gpt-oss:latest`, `gpt-oss:20b-cloud` |
| **Size**    | 모델 파일 크기 | 로컬에서 다운로드할 때 필요한 저장 용량. 단위는 GB. Cloud 모델은 로컬에 저장되지 않으므로 '-'로 표시됨 |
| **Context** | 컨텍스트 길이  | 한 번에 모델이 처리할 수 있는 최대 토큰 수. 예: 128K = 128,000 토큰             |
| **Input**   | 입력 타입    | 모델이 처리할 수 있는 입력 형태. 대부분 `Text`이며, Vision 태그가 있으면 이미지 입력도 가능 |

< 표 해석 예시 >

| Name               | Size | Context | Input | 해석                              |
| ------------------ | ---- | ------- | ----- | ------------------------------- |
| gpt-oss:latest     | 14GB | 128K    | Text  | 최신 버전 14GB, 128K 토큰 처리, 텍스트 전용  |
| gpt-oss:20b        | 14GB | 128K    | Text  | 20B 파라미터 모델, 텍스트 전용             |
| gpt-oss:120b       | 65GB | 128K    | Text  | 120B 모델, 로컬 실행 시 대용량 메모리 필요     |
| gpt-oss:20b-cloud  | -    | 128K    | Text  | 클라우드에서 실행하는 20B 모델, 로컬 다운로드 불가  |
| gpt-oss:120b-cloud | -    | 128K    | Text  | 클라우드에서 실행하는 120B 모델, 로컬 다운로드 불가 |

#### Embedding 모델의 상세보기

< 표 구성 요소 >

| 컬럼 이름       | 의미       | 상세 설명                                                                   |
| ----------- | -------- | ----------------------------------------------------------------------- |
| **Name**    | 모델 이름    | 모델 패밀리와 버전/태그를 포함합니다. 예: `embeddinggemma:latest`, `embeddinggemma:300m` |
| **Size**    | 모델 파일 크기 | 로컬 다운로드 시 필요한 저장 용량. 단위는 MB/GB                                          |
| **Context** | 컨텍스트 길이  | 한 번에 모델이 처리할 수 있는 최대 토큰 수. 임베딩 모델은 일반적으로 짧은 문장 단위(2K 토큰) 처리             |
| **Input**   | 입력 타입    | 모델이 처리할 수 있는 입력 형태. 대부분 `Text` 입력을 받음                                   |

< 표 해석 예시 >

| Name                  | Size  | Context | Input | 해석                                      |
| --------------------- | ----- | ------- | ----- | --------------------------------------- |
| embeddinggemma:latest | 622MB | 2K      | Text  | 최신 임베딩 모델, 텍스트 입력 가능, 2K 토큰 처리, 경량 모델   |
| embeddinggemma:300m   | 622MB | 2K      | Text  | 300M 파라미터 버전, 텍스트 임베딩 용도, 경량 및 빠른 실행 가능 |

#### Benchmark(벤치마크)
> **Benchmark = 기준점 / 성능 평가 시험**
> 일반적으로 **하드웨어, 소프트웨어, 모델 등의 성능을 비교·평가하기 위해 사용하는 표준 테스트**를 의미
>
> AI 모델에서는 **정해진 데이터셋과 평가 방식으로 모델 성능을 측정**하는 것  
> LLM에서는 **지식, 추론, 코드 생성, 상식 이해 등** 다양한 분야별 벤치마크가 존재
> 벤치마크 결과를 참고하면 **어떤 모델이 어떤 목적에 적합한지 판단**할 수 있음

< AI/LLM에서 Benchmark의 역할 >

| 역할            | 설명                                               |
| ------------- | ------------------------------------------------ |
| **성능 측정**     | 모델이 특정 과제를 얼마나 잘 수행하는지 수치화(정확도, F1, BLEU, MRR 등) |
| **비교 기준 제공**  | 다른 모델과 성능을 객관적으로 비교 가능                           |
| **최적화 지표**    | 모델 구조·파라미터·훈련 방법 변경 후 성능 향상 여부 확인                |
| **사용자 선택 도움** | 어떤 모델을 선택해야 하는지 판단 기준 제공                         |

< LLM 관련 벤치마크 예시 >

| Benchmark                  | 평가 대상         | 설명                        |
| -------------------------- | ------------- | ------------------------- |
| **MMLU**                   | 지식 기반 질문      | 다양한 학문 분야의 문제 해결 능력 평가    |
| **BBH**                    | 복잡한 reasoning | 다단계 추론 능력 평가              |
| **HellaSwag / StoryCloze** | 상식 추론         | 문장 이해 및 상식 추론 평가          |
| **HumanEval / MBPP**       | 코드 생성         | 코드 생성 정확도, 테스트 케이스 통과율 평가 |

< 벤치마크 결과 보는 방법 >
1. **정확도(Accuracy)**: 모델이 정답을 맞힌 비율
2. **점수(Score)**: 다양한 task 성능을 종합한 점수
3. **평균 Rank**: 여러 벤치마크에서 모델의 상대 순위
4. **추론 속도 / 비용**: 단순 성능 외에도 속도와 리소스 효율을 함께 측정

> 예:  
> `gpt-oss:20b` → MMLU 75%, BBH 62%  
> `gpt-oss:120b` → MMLU 85%, BBH 70%  
> → 파라미터가 크면 성능 향상이 있지만, 연산량·메모리 요구가 증가함

### Ollama 기본 명령어
[Ollama cli 사용 방법](https://docs.ollama.com/cli)

| 명령어                             | 설명            |
|---------------------------------|---------------|
| `ollama pull gpt-oss:20b-cloud` | 모델 다운로드       |
| `ollama list`                   | 설치된 모델 목록 확인  |
| `ollama run gpt-oss:20b-cloud`  | 모델 실행 및 대화 시작 |
| `ollama serve`                  | Ollama 서버 실행  |
| `ollama stop`                   | 실행 중인 모델 종료   |
| `ollama rm gpt-oss:20b-cloud`   | 로컬 모델 전체 삭제   | 
| `ollama rm --all`               | 로컬 모델 전체 삭제   | 


### Ollama API 사용법 이해
[ Ollama REST API 사용 방법 ](https://docs.ollama.com/api)
[ Ollama API _ OpenAI API 와 동일한 형식으로 사용 하는 방법 ](https://platform.openai.com/docs/api-reference/responses/create)
